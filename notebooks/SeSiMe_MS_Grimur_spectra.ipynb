{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Importing, first pre-processing \n",
    "Loading data, importing packages, first pre-processing steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"C:\\\\OneDrive - Netherlands eScience Center\\\\Project_Wageningen_iOMEGA\"\n",
    "PATH_MS_DATA = ROOT + \"\\\\SeSiMe\\\\data\\\\SPECTRA_grimur\\\\\"\n",
    "PATH_SAVE_MODEL = ROOT + \"\\\\SeSiMe\\\\models_trained\\\\\"\n",
    "PATH_SAVE_DATA = ROOT + \"\\\\SeSiMe\\\\data\\\\\"\n",
    "PATH_SESIME = ROOT + \"\\\\SeSiMe\\\\\"\n",
    "\n",
    "PATH_NPLINKER = ROOT + \"\\\\nplinker\\\\prototype\\\\\"\n",
    "mgf_file = PATH_MS_DATA + \"GNPSLibraries_uniqueSMILES_withFeatureIDs.mgf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import general packages\n",
    "import sys\n",
    "sys.path.insert(0, PATH_NPLINKER)\n",
    "sys.path.insert(0, PATH_SESIME)\n",
    "\n",
    "import helper_functions as functions\n",
    "import MS_functions\n",
    "\n",
    "import numpy as np\n",
    "from metabolomics import load_spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectra json file found and loaded.\n"
     ]
    }
   ],
   "source": [
    "# Import / Load data\n",
    "results_file = \"filtered_data_Grimur_minpeak10_loss500_2dec_10ppm.json\"\n",
    "\n",
    "spectra, spectra_dict, MS_documents, MS_documents_intensity, spectra_metadata = MS_functions.load_MS_data(PATH_MS_DATA, PATH_SAVE_DATA,\n",
    "                 filefilter=\"*.*\", \n",
    "                 results_file = results_file,\n",
    "                 num_decimals = 2,\n",
    "                 min_frag = 0.0, max_frag = 1000.0,\n",
    "                 min_loss = 5.0, max_loss = 500.0,\n",
    "                 min_intensity_perc = 0.0,\n",
    "                 exp_intensity_filter = 0.01,\n",
    "                 min_peaks = 10,\n",
    "                 peaks_per_mz = 15/200,\n",
    "                 merge_energies = True,\n",
    "                 merge_ppm = 10,\n",
    "                 replace = 'max',\n",
    "                 peak_loss_words = ['peak_', 'loss_'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Switch to general SeSiMe functionality\n",
    "Once we have a corpus (e.g. through cells above), we can use SeSiMe to apply different similarity measuring methds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess documents...\n",
      "Number of unique words:  35775\n"
     ]
    }
   ],
   "source": [
    "from Similarities import SimilarityMeasures\n",
    "MS_measure = SimilarityMeasures(MS_documents)\n",
    "MS_measure.preprocess_documents(0.2, min_frequency = 2, create_stopwords = False)\n",
    "print(\"Number of unique words: \", len(MS_measure.dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Train spec2vec/word2vec model and spectrum vectors\n",
    "### 1) Train on dataset itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored word2vec model not found!\n",
      "Calculating new word2vec model...\n",
      " Epoch  100  of  100 ."
     ]
    }
   ],
   "source": [
    "#file_model_word2vec = PATH_SAVE_MODEL + 'model_w2v_MS_gnps_uniquesmiles_d300_w300_iter100_loss500_minpeak10_dec2.model'\n",
    "#file_model_word2vec = PATH_SAVE_MODEL + 'model_w2v_MS_allgnps_d300_w300_iter100_loss500_minpeak10_dec2.model'\n",
    "file_model_word2vec = PATH_SAVE_MODEL + 'model_w2v_MS_Grimur_d300_w300_iter100_loss500_minpeak10_dec2.model'\n",
    "MS_measure.build_model_word2vec(file_model_word2vec, size=300, window=300, \n",
    "                             min_count=1, workers=4, iter=100, \n",
    "                             use_stored_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 'words' of the given documents were found in the trained word2vec model.\n",
      "Using present tfidf model.\n",
      "  Calculated centroid vectors for  4138  of  4138  documents."
     ]
    }
   ],
   "source": [
    "# Use peak intensities as extra weights\n",
    "MS_measure.get_vectors_centroid(method = 'update', #'ignore',\n",
    "                             extra_weights = MS_documents_intensity, \n",
    "                             tfidf_weighted = True, \n",
    "                             weight_method = None, #'sqrt', \n",
    "                             tfidf_model = None,\n",
    "                             extra_epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Calculate/load the different score matrices\n",
    "### all-vs-all matrix of spectrum-spectrum similarity scores\n",
    "+ Word2Vec-centroid similarity scores\n",
    "+ Cosine similarity scores\n",
    "+ Modified cosine scores (MolNet)\n",
    "+ Molecular similarity scores based on molecular fingerprints. Unless stated otherwise: Dice score based on morgen-3 fingerprints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate all-vs-all matrix for Word2Vec scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "M_sim_ctr = 1 - spatial.distance.cdist(MS_measure.vectors_centroid, MS_measure.vectors_centroid, 'cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate/load modified cosine score (here using \"fast\" way)\n",
    "+ Be aware: calculating those is **very slow** !\n",
    "+ Function below will load the given file and only calculate the scores from scratch if no such file is found.\n",
    "+ Method choices are 'fast' or 'hungarian', the latter being more exact but even slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = PATH_SAVE_DATA + \"MolNet_Grimur_tol02_minmatch6.npy\"\n",
    "molnet_sim = MS_functions.molnet_matrix(spectra, \n",
    "                                          tol = 0.2, \n",
    "                                          max_mz = 1000, \n",
    "                                          min_mz = 0, \n",
    "                                          min_match = 6, \n",
    "                                          min_intens = 0.01,\n",
    "                                          filename = filename,\n",
    "                                          method = 'fast', #'hungarian',\n",
    "                                          num_workers = 8,\n",
    "                                          safety_points = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate/load cosine scores \n",
    "+ Calculating is much faster than the modified cosine score, but can still become **slow**, especially when using small tolerances and little filtering (resulting in many peaks...). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find file  test0000\n",
      "Cosine scores will be calculated from scratch.\n",
      "Calculate pairwise cosine scores by  4 number of workers.\n",
      "  Calculated cosine for pair  90 -- 94 . (  100.0  % done)."
     ]
    }
   ],
   "source": [
    "filename = PATH_SAVE_DATA + \"Cosine_Grimur_tol02_minmatch6.npy\"\n",
    "cosine_sim = MS_functions.cosine_matrix(spectra[:100], \n",
    "                                      tol = 0.2, \n",
    "                                      max_mz = 1000, \n",
    "                                      min_mz = 0, \n",
    "                                      min_match = 6, \n",
    "                                      min_intens = 0,\n",
    "                                      filename = filename,\n",
    "                                      num_workers = 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate/load molecular similarity scores\n",
    "+ first calculate molecular fingerprints\n",
    "+ then calculate (or load if file exists) molecular similarity scores.  \n",
    "\n",
    "Method for calculating fingerprints here can be \"morgan1\", \"morgan2\", \"morgan3\" or \"daylight\". For morgan fingerprints scores will be based on Dice scores. For \"daylight\" fingerprint it will be Tanimoto scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecules, fingerprints_m3, exclude_IDs = MS_functions.get_mol_fingerprints(spectra_dict, method = \"morgan3\")\n",
    "exclude = [np.where(np.array(sub_spectra_metadata)[:,1] == x)[0][0] for x in exclude_IDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = PATH_SAVE_DATA + \"tanimoto_Grimur_morgan3.npy\"\n",
    "molecular_similarities = MS_functions.tanimoto_matrix(spectra, \n",
    "                                                      fingerprints_m3,\n",
    "                                                      filename = filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
